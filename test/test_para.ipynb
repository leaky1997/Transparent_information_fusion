{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/LQ1/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'params': [Parameter containing:\n",
      "tensor([-0.1677,  0.1214,  0.2673,  0.0457, -0.0326], requires_grad=True), Parameter containing:\n",
      "tensor([-0.4060, -0.1955], requires_grad=True)], 'lr': 0.0005, 'weight_decay': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None}\n",
      "{'params': [Parameter containing:\n",
      "tensor([[ 0.2264, -0.1615, -0.2389,  0.2064, -0.0162, -0.3075, -0.1194, -0.2321,\n",
      "          0.0734, -0.0276],\n",
      "        [ 0.2333,  0.1604,  0.0593,  0.1787, -0.0939,  0.2699, -0.2416, -0.0875,\n",
      "         -0.2475,  0.0998],\n",
      "        [-0.1323, -0.2750, -0.1204, -0.1857, -0.0255,  0.1255,  0.0262,  0.2441,\n",
      "         -0.1546,  0.0575],\n",
      "        [-0.2895, -0.2489, -0.2097, -0.2053, -0.1518, -0.0767,  0.2292, -0.1299,\n",
      "         -0.0530,  0.0015],\n",
      "        [ 0.1378,  0.1026, -0.0169,  0.1740, -0.1494,  0.0857,  0.1532,  0.2018,\n",
      "          0.0559,  0.0380]], requires_grad=True)], 'lr': 0.001, 'weight_decay': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None}\n",
      "{'params': [Parameter containing:\n",
      "tensor([[-0.3399,  0.2362,  0.0086, -0.2169,  0.3202],\n",
      "        [ 0.3403, -0.1006, -0.0465, -0.3259,  0.0530]], requires_grad=True)], 'lr': 0.001, 'weight_decay': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# 定义获取所有层的函数\n",
    "def get_all_layers(module, layers=None):\n",
    "    if layers is None:\n",
    "        layers = []\n",
    "    for child in module.children():\n",
    "        layers.append(child)\n",
    "        get_all_layers(child, layers)\n",
    "    return layers\n",
    "\n",
    "# 定义一个简单的网络结构\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(10, 5)\n",
    "        self.layer2 = nn.Linear(5, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        return x\n",
    "\n",
    "# 集成config_different_lr_optimizer到PyTorch Lightning模型\n",
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self, args):\n",
    "        super(LitModel, self).__init__()\n",
    "        self.network = SimpleNet()\n",
    "        self.args = args\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "    def config_different_lr_optimizer(self):\n",
    "        # 检查并设置默认的学习率\n",
    "        if not hasattr(self.args, 'learnable_parameter_learning_rate'):\n",
    "            setattr(self.args, 'learnable_parameter_learning_rate', self.args.learning_rate)\n",
    "        \n",
    "        layers = get_all_layers(self.network)\n",
    "        \n",
    "        parameters_conv = []\n",
    "        for layer in layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                # 为线性层的权重设置不同的学习率\n",
    "                parameters_conv.append({'params': layer.weight, 'lr': self.args.learning_rate, 'weight_decay': self.args.weight_decay})\n",
    "                \n",
    "        base_params = filter(lambda p: id(p) not in [id(param['params']) for param in parameters_conv], self.network.parameters())\n",
    "        \n",
    "        optimizer = Adam([\n",
    "            {'params': base_params, 'lr': self.args.learnable_parameter_learning_rate, 'weight_decay': self.args.weight_decay},\n",
    "            *parameters_conv\n",
    "        ])\n",
    "        \n",
    "        return optimizer\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = self.config_different_lr_optimizer()\n",
    "        scheduler = {\n",
    "            \"scheduler\": ReduceLROnPlateau(optimizer),\n",
    "            \"monitor\": \"val_loss\",\n",
    "            \"frequency\": 1\n",
    "        }\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
    "\n",
    "# 模拟args对象和测试函数\n",
    "class Args:\n",
    "    learning_rate = 0.001\n",
    "    weight_decay = 0.0001\n",
    "    learnable_parameter_learning_rate = 0.0005  # 假设的额外属性\n",
    "\n",
    "def test_config_different_lr_optimizer():\n",
    "    args = Args()\n",
    "    model = LitModel(args)\n",
    "    optimizer_config = model.configure_optimizers()\n",
    "    optimizer = optimizer_config[\"optimizer\"]\n",
    "    for group in optimizer.param_groups:\n",
    "        print(group)\n",
    "\n",
    "# 执行测试\n",
    "test_config_different_lr_optimizer()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LQ1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
