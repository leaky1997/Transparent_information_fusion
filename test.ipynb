{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "# 假设config是你的配置字典\n",
    "config = {\n",
    "    'args': {\n",
    "        'skip_connection': True, \n",
    "        'num_classes': 5, \n",
    "        'device': 'cuda', \n",
    "        'in_dim': 4096, \n",
    "        'out_dim': 4096, \n",
    "        'in_channels': 1, \n",
    "        'scale': 1\n",
    "    }\n",
    "}\n",
    "\n",
    "args = SimpleNamespace(**config['args'])\n",
    "\n",
    "# 现在你可以使用点符号来访问args的值\n",
    "print(args.in_dim)  # 输出：4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 绘图测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "\n",
    "class SignalProcessingLayer(nn.Module):\n",
    "    def __init__(self, signal_processing_modules, input_channels, output_channels,skip_connection=True):\n",
    "        super(SignalProcessingLayer, self).__init__()\n",
    "        self.norm = nn.InstanceNorm1d(input_channels)\n",
    "        self.weight_connection = nn.Linear(input_channels, output_channels)\n",
    "        self.signal_processing_modules = signal_processing_modules\n",
    "        self.module_num = len(signal_processing_modules)\n",
    "        if skip_connection:\n",
    "            self.skip_connection = nn.Linear(input_channels, output_channels)\n",
    "    def forward(self, x):\n",
    "        # 信号标准化\n",
    "        x = rearrange(x, 'b l c -> b c l')\n",
    "        normed_x = self.norm(x)\n",
    "        normed_x = rearrange(normed_x, 'b c l -> b l c')\n",
    "        # 通过线性层\n",
    "        x = self.weight_connection(normed_x)\n",
    "\n",
    "        # 按模块数拆分\n",
    "        splits = torch.split(x, x.size(2) // self.module_num, dim=2)\n",
    "\n",
    "        # 通过模块计算\n",
    "        outputs = []\n",
    "        for module, split in zip(self.signal_processing_modules.values(), splits):\n",
    "            outputs.append(module(split))\n",
    "        x = torch.cat(outputs, dim=2)\n",
    "        # 添加skip connection\n",
    "        if hasattr(self, 'skip_connection'):\n",
    "            x = x + self.skip_connection(normed_x)\n",
    "        return x\n",
    "    \n",
    "class FeatureExtractorlayer(nn.Module):\n",
    "    def __init__(self, feature_extractor_modules,in_channels=1, out_channels=1):\n",
    "        super(FeatureExtractorlayer, self).__init__()\n",
    "        self.weight_connection = nn.Linear(in_channels, out_channels)\n",
    "        self.feature_extractor_modules = feature_extractor_modules\n",
    "\n",
    "    def norm(self,x): # feature normalization\n",
    "        mean = x.mean(dim = 0,keepdim = True)\n",
    "        std = x.std(dim = 0,keepdim = True)\n",
    "        out = (x-mean)/(std + 1e-10)\n",
    "        return out\n",
    "           \n",
    "    def forward(self, x):\n",
    "        x = self.weight_connection(x)\n",
    "        x = rearrange(x, 'b l c -> b c l')\n",
    "        outputs = []\n",
    "        for module in self.feature_extractor_modules.values():\n",
    "            outputs.append(module(x))\n",
    "        res = torch.cat(outputs, dim=1)\n",
    "        return self.norm(res)\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes): # TODO logic\n",
    "        super(Classifier, self).__init__()\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.Linear(in_channels, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, num_classes)\n",
    "            \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.clf(x)\n",
    "\n",
    "class Transparent_Signal_Processing_Network(nn.Module):\n",
    "    def __init__(self, signal_processing_modules,feature_extractor, args):\n",
    "        super(Transparent_Signal_Processing_Network, self).__init__()\n",
    "        self.layer_num = len(signal_processing_modules)\n",
    "        self.signal_processing_modules = signal_processing_modules\n",
    "        self.feature_extractor_modules = feature_extractor\n",
    "        self.args = args\n",
    "\n",
    "        self.init_signal_processing_layers()\n",
    "        self.init_feature_extractor_layers()\n",
    "        self.init_classifier()\n",
    "\n",
    "    def init_signal_processing_layers(self):\n",
    "        print('# build signal processing layers')\n",
    "        in_channels = self.args.in_channels\n",
    "        out_channels = self.args.out_channels \n",
    "\n",
    "        self.signal_processing_layers = nn.ModuleList()\n",
    "        for i in range(self.layer_num):\n",
    "            self.signal_processing_layers.append(SignalProcessingLayer(self.signal_processing_modules[i],\n",
    "                                                                       in_channels,\n",
    "                                                                         out_channels,\n",
    "                                                                         self.args.skip_connection))\n",
    "            in_channels = out_channels \n",
    "            assert out_channels % self.signal_processing_layers[i].module_num == 0 \n",
    "            out_channels = int(out_channels * self.args.scale)\n",
    "        self.channel_for_feature = out_channels // self.args.scale\n",
    "\n",
    "    def init_feature_extractor_layers(self):\n",
    "        print('# build feature extractor layers')\n",
    "        self.feature_extractor_layers = FeatureExtractorlayer(self.feature_extractor_modules,self.channel_for_feature,self.channel_for_feature)\n",
    "        len_feature = len(self.feature_extractor_modules)\n",
    "        self.channel_for_classifier = self.channel_for_feature * len_feature\n",
    "\n",
    "\n",
    "    def init_classifier(self):\n",
    "        print('# build classifier')\n",
    "        self.clf = Classifier(self.channel_for_classifier, self.args.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        for layer in self.signal_processing_layers:\n",
    "            x = layer(x)\n",
    "        x = self.feature_extractor_layers(x)\n",
    "        x = self.clf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# build signal processing layers\n",
      "# build feature extractor layers\n",
      "# build classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/LQ1/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m net \u001b[38;5;241m=\u001b[39m Transparent_Signal_Processing_Network(signal_processing_modules,feature_extractor_modules, args)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      5\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4096\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m----> 6\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/LQ1/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/LQ1/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 111\u001b[0m, in \u001b[0;36mTransparent_Signal_Processing_Network.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignal_processing_layers:\n\u001b[0;32m--> 111\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_extractor_layers(x)\n\u001b[1;32m    113\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclf(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/LQ1/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/LQ1/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 32\u001b[0m, in \u001b[0;36mSignalProcessingLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# 添加skip connection\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskip_connection\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 32\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mskip_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormed_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "from config import args\n",
    "from config import signal_processing_modules,feature_extractor_modules\n",
    "\n",
    "net = Transparent_Signal_Processing_Network(signal_processing_modules,feature_extractor_modules, args).cuda()\n",
    "x = torch.randn(2, 4096, 2).cuda()\n",
    "y = net(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 绘图部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_signal_processing_layer(G,layer, layer_idx,input_nodes): # \n",
    "    # \n",
    "    \n",
    "    # 获取weight_connection的权重\n",
    "    weight = layer.weight_connection.weight.detach().numpy() # 获取权重\n",
    "    module_num = layer.module_num # 信号处理模块的数量\n",
    "    \n",
    "    in_channel = weight.shape[1] # 输入通道\n",
    "    out_channel = weight.shape[0] # 输出通道\n",
    "    \n",
    "    num_per_module = out_channel // module_num # 每个模块的输入通道数量\n",
    "\n",
    "\n",
    "    \n",
    "    output_nodes = [f'$x^{layer_idx + 1}_{j}$' for j in range(out_channel)]\n",
    "\n",
    "    G.add_nodes_from(output_nodes, layer='output') # 不需要隐藏掉了\n",
    "    \n",
    "\n",
    "    module_nodes = []\n",
    "    for idx, module in enumerate(layer.signal_processing_modules.values(), 1):\n",
    "        for i in range(num_per_module):\n",
    "            module_name = f'{module.name}_{idx}'\n",
    "            module_nodes.append(module_name)\n",
    "    G.add_nodes_from(module_nodes, layer='module')\n",
    "    # 添加边\n",
    "    for i, input_node in enumerate(input_nodes):\n",
    "        for j, module_node in enumerate(module_nodes):\n",
    "            # 根据权重调整边的属性   \n",
    "            G.add_edge(input_node, module_node,weight=weight[j, i])\n",
    "            G.add_edge(module_node, output_nodes[j])\n",
    "    \n",
    "    \n",
    "    # 如果存在skip_connection，则添加跳跃连接\n",
    "    if hasattr(layer, 'skip_connection'):\n",
    "        skip_weight = layer.skip_connection.weight.detach().numpy()\n",
    "        \n",
    "        # output_nodes = [f'$O_{j}$' for j in range(out_channel)] # 输出节点 \n",
    "        for i, input_node in enumerate(input_nodes):\n",
    "            for j, output_node in enumerate(output_nodes):\n",
    "                G.add_edge(input_node, output_node, weight = skip_weight[j, i], skip=True)\n",
    "    return G, output_nodes\n",
    "\n",
    "def draw_signal_processing_layers(model, input):\n",
    "    G = nx.Graph()\n",
    "    input_nodes = [f'$x^0_{j}$' for j in range(input.shape[2])]\n",
    "    for idx, layer in enumerate(model.signal_processing_layers):\n",
    "        G, input_nodes = draw_signal_processing_layer(G,layer, idx, input_nodes)    \n",
    "    \n",
    "    # 使用networkx绘制图形\n",
    "    pos = nx.spring_layout(G)  # 可以根据需要选择不同的布局\n",
    "    weights = [G[u][v]['weight'] for u, v in G.edges()]\n",
    "    nx.draw(G, pos, with_labels=True, edges=G.edges(), width=weights)\n",
    "    plt.title(f'Signal Processing Layers')\n",
    "    plt.show()\n",
    "    return G, input_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LQ1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
