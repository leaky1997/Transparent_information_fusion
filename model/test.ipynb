{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomBatchNorm(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-10):\n",
    "        super(CustomBatchNorm, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "        self.register_buffer('running_var', torch.ones(num_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            mean = x.mean(dim=0, keepdim=True)\n",
    "            var = x.var(dim=0, keepdim=True, unbiased=False)\n",
    "            self.running_mean = (1 - self.eps) * self.running_mean + self.eps * mean\n",
    "            self.running_var = (1 - self.eps) * self.running_var + self.eps * var\n",
    "            out = (x - mean) / (var.sqrt() + self.eps)\n",
    "        else:\n",
    "            out = (x - self.running_mean) / (self.running_var.sqrt() + self.eps)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 624, 1])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个随机输入张量\n",
    "input_tensor = torch.randn(64, 624, 1)\n",
    "\n",
    "# 创建一个CustomBatchNorm实例\n",
    "norm_layer = CustomBatchNorm(num_features=624)\n",
    "\n",
    "# 将输入张量传递给norm_layer\n",
    "output_tensor = norm_layer(input_tensor)\n",
    "\n",
    "# 打印输出张量的形状\n",
    "print(output_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "\n",
    "class SEAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, channel=512,reduction=4):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, l, c = x.size()\n",
    "        x = rearrange(x, 'b l c -> b c l')\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c,1)\n",
    "        res = x * y\n",
    "        return rearrange(res, 'b c l -> b l c')\n",
    "\n",
    "x = torch.randn(2, 128, 16)\n",
    "att = SEAttention(channel=16)\n",
    "y = att(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# other wavelet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sinc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sinc(band, t_right):\n",
    "    y_right = torch.sin(2 * math.pi * band * t_right) / ((2 * math.pi * band * t_right) + 1e-6)\n",
    "    y_left = torch.flip(y_right, [0])\n",
    "    y = torch.cat([y_left, torch.ones(1).to(t_right.device), y_right])\n",
    "    return y\n",
    "\n",
    "def Mexh(p):\n",
    "    # p = 0.04 * p  # 将时间转化为在[-5,5]这个区间内\n",
    "    y = (1 - torch.pow(p, 2)) * torch.exp(-torch.pow(p, 2) / 2)\n",
    "\n",
    "    return y\n",
    "\n",
    "def Laplace(p):\n",
    "    A = 0.08\n",
    "    ep = 0.03\n",
    "    tal = 0.1\n",
    "    f = 50\n",
    "    w = 2 * pi * f\n",
    "    q = torch.tensor(1 - pow(ep, 2))\n",
    "    y = A * torch.exp((-ep / (torch.sqrt(q))) * (w * (p - tal))) * (-torch.sin(w * (p - tal)))\n",
    "    return y\n",
    "\n",
    "class SincConv_multiple_channel(nn.Module):\n",
    "    def __init__(self, out_channels, kernel_size, in_channels=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        if kernel_size % 2 == 0:\n",
    "            self.kernel_size += 1\n",
    "\n",
    "        self.a_ = nn.Parameter(torch.linspace(1, 10, out_channels)).view(-1, 1)\n",
    "        self.b_ = nn.Parameter(torch.linspace(0, 10, out_channels)).view(-1, 1)\n",
    "\n",
    "    def forward(self, waveforms):\n",
    "        half_kernel = self.kernel_size // 2\n",
    "        time_disc = torch.linspace(-half_kernel, half_kernel, steps=self.kernel_size).to(waveforms.device)\n",
    "        self.a_ = self.a_.to(waveforms.device)\n",
    "        self.b_ = self.b_.to(waveforms.device)\n",
    "        \n",
    "        filters = []\n",
    "        for i in range(self.out_channels):\n",
    "            band = self.a_[i]\n",
    "            t_right = time_disc - self.b_[i]\n",
    "            filter = sinc(band, t_right)\n",
    "            filters.append(filter)\n",
    "\n",
    "        filters = torch.stack(filters)\n",
    "        self.filters = filters.view(self.out_channels, 1, -1)\n",
    "\n",
    "        output = []\n",
    "        for i in range(self.in_channels):\n",
    "            output.append(F.conv1d(waveforms[:, i:i+1], self.filters, stride=1, padding=half_kernel, dilation=1, bias=None, groups=1))\n",
    "        return torch.cat(output, dim=1)\n",
    "\n",
    "\n",
    "class Morlet_multiple_channel(nn.Module):\n",
    "\n",
    "    def __init__(self, out_channels, kernel_size, in_channels=1):\n",
    "\n",
    "        super(Morlet_multiple_channel, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size - 1\n",
    "\n",
    "        if kernel_size % 2 == 0:\n",
    "            self.kernel_size = self.kernel_size + 1\n",
    "\n",
    "        self.a_ = nn.Parameter(torch.linspace(1, 10, out_channels)).view(-1, 1)\n",
    "\n",
    "        self.b_ = nn.Parameter(torch.linspace(0, 10, out_channels)).view(-1, 1)\n",
    "\n",
    "    def forward(self, waveforms):\n",
    "\n",
    "        time_disc_right = torch.linspace(0, (self.kernel_size / 2) - 1,\n",
    "                                         steps=int((self.kernel_size / 2)))\n",
    "\n",
    "        time_disc_left = torch.linspace(-(self.kernel_size / 2) + 1, -1,\n",
    "                                        steps=int((self.kernel_size / 2)))\n",
    "\n",
    "        p1 = time_disc_right - self.b_ / self.a_\n",
    "        p2 = time_disc_left - self.b_ / self.a_\n",
    "\n",
    "        Morlet_right = Morlet(p1).to(waveforms.device)\n",
    "        Morlet_left = Morlet(p2).to(waveforms.device)\n",
    "\n",
    "        Morlet_filter = torch.cat([Morlet_left, Morlet_right], dim=1)  # 40x1x250\n",
    "\n",
    "        self.filters = (Morlet_filter).view(self.out_channels, 1, self.kernel_size).to(waveforms.device)# .cuda()\n",
    "\n",
    "        output = []\n",
    "        for i in range(self.in_channels):\n",
    "            output.append(F.conv1d(waveforms[:, i:i+1], self.filters, stride=1, padding=1, dilation=1, bias=None, groups=1))\n",
    "        return torch.cat(output, dim=1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected data shape: torch.Size([50, 10])\n",
      "Selected labels shape: (50,)\n",
      "Unique labels in selected set: [0 1 2 3 4]\n",
      "Number of samples for label 0: 10\n",
      "Number of samples for label 1: 10\n",
      "Number of samples for label 2: 10\n",
      "Number of samples for label 3: 10\n",
      "Number of samples for label 4: 10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 假设的 select_validation_samples 函数\n",
    "def select_validation_samples(data_all, label_all, num_samples):\n",
    "    unique_labels = np.unique(label_all)\n",
    "    indices_to_keep = []\n",
    "\n",
    "    for label in unique_labels:            \n",
    "        indices = np.where(label_all == label)[0]\n",
    "        if len(indices) > num_samples:\n",
    "            chosen_indices = indices[:num_samples]\n",
    "        else:\n",
    "            chosen_indices = indices\n",
    "        indices_to_keep.extend(chosen_indices)\n",
    "\n",
    "    return data_all[indices_to_keep], label_all[indices_to_keep]\n",
    "\n",
    "# 生成模拟数据和标签\n",
    "np.random.seed(0)  # 为了可重复性\n",
    "data_all = torch.randn(100, 10)  # 假设有100个样本，每个样本10个特征\n",
    "label_all = np.random.randint(0, 5, size=(100,))  # 假设有5个类别\n",
    "\n",
    "# 调用函数\n",
    "selected_data, selected_labels = select_validation_samples(data_all, label_all, 10)\n",
    "\n",
    "# 打印结果\n",
    "print(\"Selected data shape:\", selected_data.shape)\n",
    "print(\"Selected labels shape:\", selected_labels.shape)\n",
    "print(\"Unique labels in selected set:\", np.unique(selected_labels))\n",
    "\n",
    "# 验证每个类别的样本数是否正确\n",
    "for label in np.unique(label_all):\n",
    "    print(f\"Number of samples for label {label}: {np.sum(selected_labels == label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 512, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from einops import rearrange\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import init\n",
    "\n",
    "\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self,channel,reduction=16):\n",
    "        super().__init__()\n",
    "        self.maxpool=nn.AdaptiveMaxPool1d(1)\n",
    "        self.avgpool=nn.AdaptiveAvgPool1d(1)\n",
    "        self.se=nn.Sequential(\n",
    "            nn.Conv1d(channel,channel//reduction,1,bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(channel//reduction,channel,1,bias=False)\n",
    "        )\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x) :\n",
    "        max_result=self.maxpool(x)\n",
    "        avg_result=self.avgpool(x)\n",
    "        max_out=self.se(max_result)\n",
    "        avg_out=self.se(avg_result)\n",
    "        output=self.softmax(max_out+avg_out)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "input=torch.randn(50,512,6) # B,C,L\n",
    "kernel_size= 7\n",
    "cbam = ChannelAttention(channel=512,reduction=16)\n",
    "output=cbam(input)\n",
    "print(output.shape)\n",
    "\n",
    "    \n",
    "       \n",
    "# batch_size, seq_length, channel = 10, 512, 6\n",
    "# x = torch.randn(batch_size, seq_length, channel)  # 创建一个随机输入张量\n",
    "\n",
    "# attention_layer = SPAttention(channel=channel, reduction=8)  # 实例化注意力层\n",
    "# output = attention_layer(x)  # 前向传播\n",
    "\n",
    "# print(f\"Input shape: {x.shape}\")\n",
    "# print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# # 检查输出形状是否正确\n",
    "# assert output.shape == x.shape, f\"Expected output shape {x.shape}, but got {output.shape}\"\n",
    "\n",
    "# print(\"Test passed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000],\n",
      "         [0.0000],\n",
      "         [0.1000],\n",
      "         ...,\n",
      "         [0.1000],\n",
      "         [0.1000],\n",
      "         [0.1000]],\n",
      "\n",
      "        [[0.0995],\n",
      "         [0.0000],\n",
      "         [0.1005],\n",
      "         ...,\n",
      "         [0.0994],\n",
      "         [0.0000],\n",
      "         [0.0000]],\n",
      "\n",
      "        [[0.0993],\n",
      "         [0.0000],\n",
      "         [0.1006],\n",
      "         ...,\n",
      "         [0.0992],\n",
      "         [0.0000],\n",
      "         [0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0000],\n",
      "         [0.0000],\n",
      "         [0.1000],\n",
      "         ...,\n",
      "         [0.1000],\n",
      "         [0.1000],\n",
      "         [0.1000]],\n",
      "\n",
      "        [[0.0999],\n",
      "         [0.0000],\n",
      "         [0.1001],\n",
      "         ...,\n",
      "         [0.0999],\n",
      "         [0.0000],\n",
      "         [0.0000]],\n",
      "\n",
      "        [[0.0000],\n",
      "         [0.0000],\n",
      "         [0.1000],\n",
      "         ...,\n",
      "         [0.1000],\n",
      "         [0.1000],\n",
      "         [0.1000]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, channel, reduction=16, topk=10):\n",
    "        super().__init__()\n",
    "        # self.maxpool=nn.AdaptiveMaxPool1d(1) # B,C,L -> B,C,1\n",
    "        self.varpool = lambda x: (((x - torch.mean(x, dim=-1, keepdim=True)) ** 2).mean(dim=-1, keepdim=True))\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.se1 = nn.Sequential(\n",
    "            nn.Conv1d(channel, channel // reduction, 1, bias=False), # = linear\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(channel // reduction, channel, 1, bias=False)\n",
    "        )\n",
    "        self.se2 = nn.Sequential(\n",
    "            nn.Conv1d(channel, channel // reduction, 1, bias=False), # = linear\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(channel // reduction, channel, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.topk = topk\n",
    "\n",
    "    def sparser(self, x, topk):\n",
    "        # Get the topk values and their indices\n",
    "        topk_values, topk_indices = torch.topk(x, topk, dim=1)\n",
    "        # Create a mask for the topk values\n",
    "        mask = torch.zeros_like(x, dtype=torch.bool)\n",
    "        mask.scatter_(1, topk_indices, True)\n",
    "        # Set the values not in the topk to negative infinity\n",
    "        x[~mask] = float('-inf')\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = rearrange(x, 'b l c -> b c l')\n",
    "        var_result = self.varpool(x)\n",
    "        avg_result = self.avgpool(x)\n",
    "        var_out = self.se1(var_result)\n",
    "        avg_out = self.se2(avg_result)\n",
    "        res = var_out + avg_out\n",
    "        res = self.sparser(res, self.topk)\n",
    "        # output=self.sigmoid(max_out+avg_out)\n",
    "        output = self.softmax(res)\n",
    "        return output\n",
    "\n",
    "# Test the module with a random input\n",
    "x = torch.randn(100, 4096, 16)  # Example input tensor # B,L,C\n",
    "channel_attention = ChannelAttention(channel=16)\n",
    "output = channel_attention(x)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "Original Tensor:\n",
      " tensor([[[ 0.4005, -0.7963,  0.5716],\n",
      "         [ 0.1521, -0.8838,  1.3007]]])\n",
      "Shuffled Tensor:\n",
      " tensor([[[ 0.4005, -0.7963,  0.5716],\n",
      "         [ 0.1521, -0.8838,  1.3007]]])\n",
      "Cosine Similarity:\n",
      " tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def random_shuffle_channel(tensor, channel_index):\n",
    "    # 提取要打乱的通道\n",
    "    channel = tensor[:, channel_index, :].clone()\n",
    "    \n",
    "    # 随机打乱通道\n",
    "    perm = torch.randperm(channel.size(0))\n",
    "    shuffled_channel = channel[perm]\n",
    "    \n",
    "    # 将打乱后的通道重新放回tensor\n",
    "    new_tensor = tensor.clone()\n",
    "    new_tensor[:, channel_index, :] = shuffled_channel\n",
    "    \n",
    "    return new_tensor\n",
    "\n",
    "def cosine_similarity(x, y):\n",
    "    return F.cosine_similarity(x, y, dim=-1)\n",
    "\n",
    "# 示例用法\n",
    "# 假设输入tensor的形状为(batch_size, num_channels, height, width)\n",
    "tensor = torch.randn(1, 2, 3)\n",
    "\n",
    "# 随机打乱C通道（假设C通道索引为2）\n",
    "shuffled_tensor = random_shuffle_channel(tensor, 1)\n",
    "\n",
    "# 计算原始tensor与打乱后tensor之间的余弦相似度\n",
    "similarity = cosine_similarity(tensor.view(tensor.size(0), -1), shuffled_tensor.view(shuffled_tensor.size(0), -1))\n",
    "\n",
    "print(similarity)\n",
    "\n",
    "print(\"Original Tensor:\\n\", tensor)\n",
    "print(\"Shuffled Tensor:\\n\", shuffled_tensor)\n",
    "print(\"Cosine Similarity:\\n\", similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor:\n",
      " tensor([[[-1.5382],\n",
      "         [-0.6274],\n",
      "         [ 0.2217]],\n",
      "\n",
      "        [[-1.2906],\n",
      "         [-1.5065],\n",
      "         [ 0.0462]],\n",
      "\n",
      "        [[ 0.8653],\n",
      "         [-0.1549],\n",
      "         [ 0.8502]],\n",
      "\n",
      "        [[-0.9844],\n",
      "         [ 0.1447],\n",
      "         [ 0.4886]],\n",
      "\n",
      "        [[-0.7618],\n",
      "         [-1.5920],\n",
      "         [-0.9459]]])\n",
      "Shuffled Tensor:\n",
      " tensor([[[-0.6274],\n",
      "         [-1.5382],\n",
      "         [ 0.2217]],\n",
      "\n",
      "        [[-1.5065],\n",
      "         [-1.2906],\n",
      "         [ 0.0462]],\n",
      "\n",
      "        [[-0.1549],\n",
      "         [ 0.8653],\n",
      "         [ 0.8502]],\n",
      "\n",
      "        [[ 0.1447],\n",
      "         [-0.9844],\n",
      "         [ 0.4886]],\n",
      "\n",
      "        [[-1.5920],\n",
      "         [-0.7618],\n",
      "         [-0.9459]]])\n",
      "Cosine Similarity:\n",
      " tensor([[ 0.7046],\n",
      "        [ 0.9882],\n",
      "        [ 0.3040],\n",
      "        [-0.0376],\n",
      "        [ 0.8281]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def random_shuffle_channels(tensor):\n",
    "    # 获取C通道的数量\n",
    "    C = tensor.size(1)\n",
    "    \n",
    "    # 生成随机的C通道索引\n",
    "    perm = torch.randperm(C)\n",
    "    \n",
    "    # 打乱C通道\n",
    "    shuffled_tensor = tensor[:, perm]\n",
    "    \n",
    "    return shuffled_tensor\n",
    "\n",
    "def cosine_similarity(x, y):\n",
    "    return F.cosine_similarity(x, y, dim=1)\n",
    "\n",
    "# 示例输入\n",
    "B, C = 5, 3  # B是样本数量，C是通道数量\n",
    "tensor = torch.randn(B, C,1)\n",
    "\n",
    "# 打乱C通道\n",
    "shuffled_tensor = random_shuffle_channels(tensor)\n",
    "\n",
    "# 计算余弦相似度\n",
    "# 这里假设我们想计算tensor和打乱后的shuffled_tensor之间的相似度\n",
    "cos_sim = cosine_similarity(tensor, shuffled_tensor)\n",
    "\n",
    "print(\"Original Tensor:\\n\", tensor)\n",
    "print(\"Shuffled Tensor:\\n\", shuffled_tensor)\n",
    "print(\"Cosine Similarity:\\n\", cos_sim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test DEN opearator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed.\n",
      "All tests passed.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sympy\n",
    "from Signal_processing import SignalProcessingBase\n",
    "from einops import rearrange\n",
    "\n",
    "KERNEL_SIZE = 49\n",
    "FRE = 10\n",
    "DEVICE = 'cuda'\n",
    "STRIDE = 1\n",
    "T = torch.linspace(-KERNEL_SIZE / 2, KERNEL_SIZE / 2, KERNEL_SIZE).view(1, 1, KERNEL_SIZE).to(DEVICE)\n",
    "\n",
    "def Morlet(t):\n",
    "    C = pow(math.pi, 0.25)\n",
    "    f = FRE\n",
    "    w = 2 * math.pi * f    \n",
    "    y = C * torch.exp(-torch.pow(t, 2) / 2) * torch.cos(w * t)\n",
    "    return y\n",
    "\n",
    "def Laplace(t):\n",
    "    a = 0.08\n",
    "    ep = 0.03\n",
    "    tal = 0.1\n",
    "    f = FRE\n",
    "    w = 2 * math.pi * f\n",
    "    q = torch.tensor(1 - pow(ep, 2))\n",
    "    y = a * torch.exp((-ep / (torch.sqrt(q))) * (w * (t - tal))) * (-torch.sin(w * (t - tal)))\n",
    "    return y\n",
    "\n",
    "class convlutional_operator(nn.Module):\n",
    "    def __init__(self, kernel_op='conv_sin', dim=1, stride=STRIDE, kernel_size=KERNEL_SIZE, device='cuda', in_channels=1):\n",
    "        super().__init__()\n",
    "        self.affline = nn.InstanceNorm1d(num_features=dim, affine=True).to(device)\n",
    "        op_dic = {'conv_sin': torch.sin,\n",
    "                  'conv_sin2': lambda x: torch.sin(x ** 2),\n",
    "                  'conv_exp': torch.exp,\n",
    "                  'conv_exp2': lambda x: torch.exp(x ** 2),\n",
    "                  'Morlet': Morlet,\n",
    "                  'Laplace': Laplace}\n",
    "        self.op = op_dic[kernel_op]\n",
    "        self.stride = stride\n",
    "        self.in_channels = in_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.t = torch.linspace(-math.pi / 2, math.pi / 2, kernel_size).view(1, 1, kernel_size).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = rearrange(x, 'b l c -> b c l')\n",
    "        self.aff_t = self.affline(self.t)\n",
    "        self.weight = self.op(self.aff_t).view(1, 1, -1).repeat(self.in_channels, 1, 1)\n",
    "        conv = F.conv1d(x, self.weight, stride=self.stride, padding=(self.kernel_size - 1) // 2, dilation=1, groups=self.in_channels)\n",
    "        conv = rearrange(conv, 'b c l -> b l c')\n",
    "        return conv\n",
    "\n",
    "class signal_filter_(nn.Module):\n",
    "    def __init__(self, kernel_op='order1_MA', dim=1, stride=STRIDE, kernel_size=KERNEL_SIZE, device='cuda', in_channels=1):\n",
    "        super().__init__()\n",
    "        self.affline = nn.InstanceNorm1d(num_features=dim, affine=True).to(device)\n",
    "        op_dic = {'order1_MA': torch.tensor([0.5, 0, 0.5]),\n",
    "                  'order2_MA': torch.tensor([1 / 3, 1 / 3, 1 / 3]),\n",
    "                  'order1_DF': torch.tensor([-1.0, 0, 1.0]),\n",
    "                  'order2_DF': torch.tensor([-1.0, 2.0, -1.0])}\n",
    "        self.weight = op_dic[kernel_op].view(1, 1, -1).to(device).repeat(in_channels, 1, 1)\n",
    "        self.stride = stride\n",
    "        self.kernel_size = 3\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = rearrange(x, 'b l c -> b c l')\n",
    "        conv = F.conv1d(x, self.weight, stride=self.stride, padding=(self.kernel_size - 1) // 2, dilation=1, groups=self.in_channels)\n",
    "        conv = rearrange(conv, 'b c l -> b l c')\n",
    "        return conv\n",
    "\n",
    "class MorletFilter(SignalProcessingBase):\n",
    "    def __init__(self, args):\n",
    "        super(MorletFilter, self).__init__(args)\n",
    "        self.name = \"Morlet\"\n",
    "        self.convolution_operator = convlutional_operator('Morlet', in_channels=args.scale, device=args.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_transformed = self.convolution_operator(x)\n",
    "        return x_transformed\n",
    "\n",
    "class LaplaceFilter(SignalProcessingBase):\n",
    "    def __init__(self, args):\n",
    "        super(LaplaceFilter, self).__init__(args)\n",
    "        self.name = \"Laplace\"\n",
    "        self.convolution_operator = convlutional_operator('Laplace', in_channels=args.scale, device=args.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_transformed = self.convolution_operator(x)\n",
    "        return x_transformed\n",
    "\n",
    "class Order1MAFilter(SignalProcessingBase):\n",
    "    def __init__(self, args):\n",
    "        super(Order1MAFilter, self).__init__(args)\n",
    "        self.name = \"order1_MA\"\n",
    "        self.filter_operator = signal_filter_('order1_MA', in_channels=args.scale, device=args.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_transformed = self.filter_operator(x)\n",
    "        return x_transformed\n",
    "\n",
    "class Order2MAFilter(SignalProcessingBase):\n",
    "    def __init__(self, args):\n",
    "        super(Order2MAFilter, self).__init__(args)\n",
    "        self.name = \"order2_MA\"\n",
    "        self.filter_operator = signal_filter_('order2_MA', in_channels=args.scale, device=args.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_transformed = self.filter_operator(x)\n",
    "        return x_transformed\n",
    "\n",
    "class Order1DFFilter(SignalProcessingBase):\n",
    "    def __init__(self, args):\n",
    "        super(Order1DFFilter, self).__init__(args)\n",
    "        self.name = \"order1_DF\"\n",
    "        self.filter_operator = signal_filter_('order1_DF', in_channels=args.scale, device=args.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_transformed = self.filter_operator(x)\n",
    "        return x_transformed\n",
    "\n",
    "class Order2DFFilter(SignalProcessingBase):\n",
    "    def __init__(self, args):\n",
    "        super(Order2DFFilter, self).__init__(args)\n",
    "        self.name = \"order2_DF\"\n",
    "        self.filter_operator = signal_filter_('order2_DF', in_channels=args.scale, device=args.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_transformed = self.filter_operator(x)\n",
    "        return x_transformed\n",
    "\n",
    "class LogOperation(SignalProcessingBase):\n",
    "    def __init__(self, args):\n",
    "        super(LogOperation, self).__init__(args)\n",
    "        self.name = \"log\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.log(x)\n",
    "class SquOperation(SignalProcessingBase):\n",
    "    def __init__(self, args):\n",
    "        super(SquOperation, self).__init__(args)\n",
    "        self.name = \"squ\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x ** 2\n",
    "\n",
    "class SinOperation(SignalProcessingBase):\n",
    "    def __init__(self, args):\n",
    "        super(SinOperation, self).__init__(args)\n",
    "        self.name = \"sin\"\n",
    "        self.fre = FRE # TODO learbable\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sin(self.fre * x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SignalProcessingBase(torch.nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(SignalProcessingBase, self).__init__()\n",
    "        self.args = args\n",
    "        self.in_dim = args.in_dim\n",
    "        self.out_dim = args.out_dim\n",
    "        self.in_channels = args.in_channels\n",
    "        self.out_channels = args.out_channels\n",
    "        self.device = args.device\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError(\"This method should be implemented by subclass.\")\n",
    "    \n",
    "    def test_forward(self):\n",
    "        test_input = torch.randn(2, self.in_dim, self.in_channels).to(self.device)\n",
    "        output = self.forward(test_input)\n",
    "        assert output.shape == (2, self.out_dim, self.out_channels), f\"\\\n",
    "        input shape is {test_input.shape}, \\n\\\n",
    "        Output shape is {output.shape}, \\n\\\n",
    "        expected {(2, self.out_dim, self.out_channels)}\"\n",
    "\n",
    "class SignalProcessingModuleDict(torch.nn.ModuleDict):\n",
    "    def __init__(self, module_dict):\n",
    "        super(SignalProcessingModuleDict, self).__init__(module_dict)\n",
    "\n",
    "    def forward(self, x, key):\n",
    "        if key in self:\n",
    "            return self[key](x)\n",
    "        else:\n",
    "            raise KeyError(f\"No signal processing module found for key: {key}\")\n",
    "        \n",
    "    def test_forward(self):\n",
    "        for key in self.keys():\n",
    "            self[key].test_forward()\n",
    "class Args:\n",
    "    def __init__(self, in_dim, out_dim, in_channels, out_channels, device, scale):\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.device = device\n",
    "        self.scale = scale\n",
    "        self.f_c_mu = 0\n",
    "        self.f_c_sigma = 1\n",
    "        self.f_b_mu = 0\n",
    "        self.f_b_sigma = 1\n",
    "\n",
    "# 设置测试参数\n",
    "args = Args(in_dim=49, out_dim=49, in_channels=3, out_channels=3, device='cuda', scale=3)\n",
    "\n",
    "# 创建过滤器实例\n",
    "morlet_filter = MorletFilter(args)\n",
    "laplace_filter = LaplaceFilter(args)\n",
    "order1_ma_filter = Order1MAFilter(args)\n",
    "order2_ma_filter = Order2MAFilter(args)\n",
    "order1_df_filter = Order1DFFilter(args)\n",
    "order2_df_filter = Order2DFFilter(args)\n",
    "\n",
    "# 创建模块字典\n",
    "module_dict = {\n",
    "    \"Morlet\": morlet_filter,\n",
    "    \"Laplace\": laplace_filter,\n",
    "    \"order1_MA\": order1_ma_filter,\n",
    "    \"order2_MA\": order2_ma_filter,\n",
    "    \"order1_DF\": order1_df_filter,\n",
    "    \"order2_DF\": order2_df_filter\n",
    "}\n",
    "\n",
    "spm_dict = SignalProcessingModuleDict(module_dict)\n",
    "\n",
    "# 测试所有模块\n",
    "spm_dict.test_forward()\n",
    "\n",
    "print(\"All tests passed.\")\n",
    "\n",
    "\n",
    "# 创建操作实例\n",
    "log_op = LogOperation(args)\n",
    "squ_op = SquOperation(args)\n",
    "sin_op = SinOperation(args)\n",
    "\n",
    "# 测试所有操作\n",
    "log_op.test_forward()\n",
    "squ_op.test_forward()\n",
    "sin_op.test_forward()\n",
    "\n",
    "print(\"All tests passed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_input test passed for add\n",
      "forward test passed for add\n",
      "split_input test passed for mul\n",
      "forward test passed for mul\n",
      "split_input test passed for div\n",
      "forward test passed for div\n",
      "All tests passed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 定义基类和各个操作类\n",
    "class SignalProcessingBase2Arity(torch.nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(SignalProcessingBase2Arity, self).__init__()\n",
    "        self.args = args\n",
    "        self.in_dim = args.in_dim\n",
    "        self.out_dim = args.out_dim\n",
    "        self.in_channels = args.in_channels\n",
    "        self.out_channels = args.out_channels\n",
    "        self.device = args.device\n",
    "        self.to(self.device)\n",
    "\n",
    "    def split_input(self, x):\n",
    "        # 拆分输入信号\n",
    "        half_channels = self.in_channels // 2\n",
    "        x1 = x[:, :, :half_channels]\n",
    "        x2 = x[:, :, half_channels:]\n",
    "        return x1, x2\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, x2 = self.split_input(x)\n",
    "        return self.operation(x1, x2)\n",
    "\n",
    "    def operation(self, x1, x2):\n",
    "        raise NotImplementedError(\"This method should be implemented by subclass.\")\n",
    "    \n",
    "    def test_forward(self):\n",
    "        test_input = torch.randn(2, self.in_dim, self.in_channels).to(self.device)\n",
    "        output = self.forward(test_input)\n",
    "        assert output.shape == (2, self.out_dim, self.out_channels), f\"\\\n",
    "        input shape is {test_input.shape}, \\n\\\n",
    "        Output shape is {output.shape}, \\n\\\n",
    "        expected {(2, self.out_dim, self.out_channels)}\"\n",
    "\n",
    "class AddOperation(SignalProcessingBase2Arity):\n",
    "    def __init__(self, args):\n",
    "        super(AddOperation, self).__init__(args)\n",
    "        self.name = \"add\"\n",
    "\n",
    "    def operation(self, x1, x2):\n",
    "        return x1 + x2\n",
    "\n",
    "class MulOperation(SignalProcessingBase2Arity):\n",
    "    def __init__(self, args):\n",
    "        super(MulOperation, self).__init__(args)\n",
    "        self.name = \"mul\"\n",
    "\n",
    "    def operation(self, x1, x2):\n",
    "        return x1 * x2\n",
    "\n",
    "class DivOperation(SignalProcessingBase2Arity):\n",
    "    def __init__(self, args):\n",
    "        super(DivOperation, self).__init__(args)\n",
    "        self.name = \"div\"\n",
    "\n",
    "    def operation(self, x1, x2):\n",
    "        return x1 / x2\n",
    "\n",
    "# 设置测试参数\n",
    "class Args:\n",
    "    def __init__(self, in_dim, out_dim, in_channels, out_channels, device, scale):\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.device = device\n",
    "        self.scale = scale\n",
    "\n",
    "args = Args(in_dim=49, out_dim=49, in_channels=4, out_channels=2, device='cuda', scale=3)\n",
    "\n",
    "# 创建操作实例\n",
    "add_op = AddOperation(args)\n",
    "mul_op = MulOperation(args)\n",
    "div_op = DivOperation(args)\n",
    "\n",
    "# 测试 split_input 方法\n",
    "def test_split_input(operation):\n",
    "    test_input = torch.randn(2, args.in_dim, args.in_channels).to(args.device)\n",
    "    x1, x2 = operation.split_input(test_input)\n",
    "    assert x1.shape == (2, args.in_dim, args.in_channels // 2), f\"Expected shape: {(2, args.in_dim, args.in_channels // 2)}, but got {x1.shape}\"\n",
    "    assert x2.shape == (2, args.in_dim, args.in_channels // 2), f\"Expected shape: {(2, args.in_dim, args.in_channels // 2)}, but got {x2.shape}\"\n",
    "    print(f\"split_input test passed for {operation.name}\")\n",
    "\n",
    "# 测试 forward 方法\n",
    "def test_forward(operation):\n",
    "    operation.test_forward()\n",
    "    print(f\"forward test passed for {operation.name}\")\n",
    "\n",
    "# 运行测试\n",
    "for op in [add_op, mul_op, div_op]:\n",
    "    test_split_input(op)\n",
    "    test_forward(op)\n",
    "\n",
    "print(\"All tests passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试逻辑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "ONE = torch.Tensor([1]).cuda()\n",
    "ZERO = torch.Tensor([0]).cuda()\n",
    "\n",
    "class LogicInferenceBase(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(LogicInferenceBase, self).__init__()\n",
    "        self.args = args\n",
    "        self.in_dim = args.in_dim\n",
    "        self.out_dim = args.out_dim\n",
    "        self.in_channels = args.in_channels\n",
    "        self.out_channels = args.out_channels\n",
    "        self.device = args.device\n",
    "        self.to(self.device)\n",
    "        \n",
    "    @staticmethod\n",
    "    def generalized_softmax(x, y, alpha=20):\n",
    "        numerator = x * torch.exp(alpha * x) + y * torch.exp(alpha * y)\n",
    "        denominator = torch.exp(alpha * x) + torch.exp(alpha * y)\n",
    "        return numerator / denominator \n",
    "\n",
    "    @staticmethod\n",
    "    def generalized_softmin(x, y, alpha=20):\n",
    "        return -LogicInferenceBase.generalized_softmax(-x, -y, alpha=alpha)\n",
    "\n",
    "    @staticmethod\n",
    "    def implication(x, y):\n",
    "        return LogicInferenceBase.generalized_softmin(ONE, ONE - x + y)\n",
    "\n",
    "    @staticmethod\n",
    "    def equivalence(x, y):\n",
    "        return ONE - torch.abs(x - y)\n",
    "\n",
    "    @staticmethod\n",
    "    def negation(x):\n",
    "        return ONE - x\n",
    "\n",
    "    @staticmethod\n",
    "    def weak_conjunction(x, y):\n",
    "        return LogicInferenceBase.generalized_softmin(x, y)\n",
    "\n",
    "    @staticmethod\n",
    "    def weak_disjunction(x, y):\n",
    "        return LogicInferenceBase.generalized_softmax(x, y)\n",
    "\n",
    "    @staticmethod\n",
    "    def strong_conjunction(x, y):\n",
    "        return LogicInferenceBase.generalized_softmax(ZERO, x + y - 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def strong_disjunction(x, y):\n",
    "        return LogicInferenceBase.generalized_softmin(ONE, x + y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "ONE = torch.Tensor([1]).cuda()\n",
    "ZERO = torch.Tensor([0]).cuda()\n",
    "\n",
    "class LogicInferenceBase(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(LogicInferenceBase, self).__init__()\n",
    "        self.args = args\n",
    "        self.in_channels = args.in_channels\n",
    "        self.out_channels = args.out_channels\n",
    "        self.device = args.device\n",
    "        self.to(self.device)\n",
    "        \n",
    "    @staticmethod\n",
    "    def generalized_softmax(x, y, alpha=20):\n",
    "        numerator = x * torch.exp(alpha * x) + y * torch.exp(alpha * y)\n",
    "        denominator = torch.exp(alpha * x) + torch.exp(alpha * y)\n",
    "        return numerator / denominator \n",
    "\n",
    "    @staticmethod\n",
    "    def generalized_softmin(x, y, alpha=20):\n",
    "        return -LogicInferenceBase.generalized_softmax(-x, -y, alpha=alpha)\n",
    "\n",
    "    @staticmethod\n",
    "    def implication(x, y):\n",
    "        return LogicInferenceBase.generalized_softmin(ONE, ONE - x + y)\n",
    "\n",
    "    @staticmethod\n",
    "    def equivalence(x, y):\n",
    "        return ONE - torch.abs(x - y)\n",
    "\n",
    "    @staticmethod\n",
    "    def negation(x):\n",
    "        return ONE - x\n",
    "\n",
    "    @staticmethod\n",
    "    def weak_conjunction(x, y):\n",
    "        return LogicInferenceBase.generalized_softmin(x, y)\n",
    "\n",
    "    @staticmethod\n",
    "    def weak_disjunction(x, y):\n",
    "        return LogicInferenceBase.generalized_softmax(x, y)\n",
    "\n",
    "    @staticmethod\n",
    "    def strong_conjunction(x, y):\n",
    "        return LogicInferenceBase.generalized_softmax(ZERO, x + y - 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def strong_disjunction(x, y):\n",
    "        return LogicInferenceBase.generalized_softmin(ONE, x + y)\n",
    "    def test_forward(self):\n",
    "        test_input = torch.randn(2, self.in_channels).to(self.device)\n",
    "        output = self.forward(test_input)\n",
    "        assert output.shape == (2, self.out_channels), f\"\\\n",
    "        input shape is {test_input.shape}, \\n\\\n",
    "        Output shape is {output.shape}, \\n\\\n",
    "        expected {(2, self.out_channels)}\"\n",
    "        \n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError(\"This method should be implemented by subclass.\")\n",
    "\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogicInferenceBase2Arity(LogicInferenceBase):\n",
    "    def __init__(self, args):\n",
    "        super(LogicInferenceBase2Arity, self).__init__(args)\n",
    "        \n",
    "    def split_input(self, x):\n",
    "        # 拆分输入信号\n",
    "        half_channels = self.in_channels // 2\n",
    "        x1 = x[:, :half_channels]\n",
    "        x2 = x[:, half_channels:]\n",
    "        return x1, x2\n",
    "    def repeat_input(self, x):\n",
    "        return torch.cat([x, x], dim=-1)\n",
    "    def forward(self, x):\n",
    "        x1, x2 = self.split_input(x)\n",
    "        x = self.operation(x1, x2)\n",
    "        x = self.repeat_input(x)\n",
    "        return \n",
    "\n",
    "    def operation(self, x1, x2):\n",
    "        raise NotImplementedError(\"This method should be implemented by subclass.\")\n",
    "    \n",
    "    def test_forward(self):\n",
    "        test_input = torch.randn(2, self.in_channels).to(self.device)\n",
    "        output = self.forward(test_input)\n",
    "        assert output.shape == (2, self.out_channels), f\"\\\n",
    "        input shape is {test_input.shape}, \\n\\\n",
    "        Output shape is {output.shape}, \\n\\\n",
    "        expected {(2, self.out_channels)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImplicationOperation(LogicInferenceBase2Arity):\n",
    "    def __init__(self, args):\n",
    "        super(ImplicationOperation, self).__init__(args)\n",
    "        self.name = \"implication\"\n",
    "\n",
    "    def operation(self, x1, x2):\n",
    "        return LogicInferenceBase.implication(x1, x2)\n",
    "\n",
    "class EquivalenceOperation(LogicInferenceBase2Arity):\n",
    "    def __init__(self, args):\n",
    "        super(EquivalenceOperation, self).__init__(args)\n",
    "        self.name = \"equivalence\"\n",
    "\n",
    "    def operation(self, x1, x2):\n",
    "        return LogicInferenceBase.equivalence(x1, x2)\n",
    "\n",
    "class NegationOperation(LogicInferenceBase):\n",
    "    def __init__(self, args):\n",
    "        super(NegationOperation, self).__init__(args)\n",
    "        self.name = \"negation\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 对于 negation，只使用 x1\n",
    "        return LogicInferenceBase.negation(x)\n",
    "\n",
    "class WeakConjunctionOperation(LogicInferenceBase2Arity):\n",
    "    def __init__(self, args):\n",
    "        super(WeakConjunctionOperation, self).__init__(args)\n",
    "        self.name = \"weak_conjunction\"\n",
    "\n",
    "    def operation(self, x1, x2):\n",
    "        return LogicInferenceBase.weak_conjunction(x1, x2)\n",
    "\n",
    "class WeakDisjunctionOperation(LogicInferenceBase2Arity):\n",
    "    def __init__(self, args):\n",
    "        super(WeakDisjunctionOperation, self).__init__(args)\n",
    "        self.name = \"weak_disjunction\"\n",
    "\n",
    "    def operation(self, x1, x2):\n",
    "        return LogicInferenceBase.weak_disjunction(x1, x2)\n",
    "\n",
    "class StrongConjunctionOperation(LogicInferenceBase2Arity):\n",
    "    def __init__(self, args):\n",
    "        super(StrongConjunctionOperation, self).__init__(args)\n",
    "        self.name = \"strong_conjunction\"\n",
    "\n",
    "    def operation(self, x1, x2):\n",
    "        return LogicInferenceBase.strong_conjunction(x1, x2)\n",
    "\n",
    "class StrongDisjunctionOperation(LogicInferenceBase2Arity):\n",
    "    def __init__(self, args):\n",
    "        super(StrongDisjunctionOperation, self).__init__(args)\n",
    "        self.name = \"strong_disjunction\"\n",
    "\n",
    "    def operation(self, x1, x2):\n",
    "        return LogicInferenceBase.strong_disjunction(x1, x2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_input test passed for implication\n",
      "forward test passed for implication\n",
      "split_input test passed for equivalence\n",
      "forward test passed for equivalence\n",
      "forward test passed for negation\n",
      "split_input test passed for weak_conjunction\n",
      "forward test passed for weak_conjunction\n",
      "split_input test passed for weak_disjunction\n",
      "forward test passed for weak_disjunction\n",
      "split_input test passed for strong_conjunction\n",
      "forward test passed for strong_conjunction\n",
      "split_input test passed for strong_disjunction\n",
      "forward test passed for strong_disjunction\n",
      "All tests passed.\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self, in_channels, out_channels, device):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.device = device\n",
    "\n",
    "args = Args(in_channels=4, out_channels=4, device='cuda')\n",
    "\n",
    "# 创建操作实例\n",
    "implication_op = ImplicationOperation(args)\n",
    "equivalence_op = EquivalenceOperation(args)\n",
    "negation_op = NegationOperation(args)\n",
    "weak_conjunction_op = WeakConjunctionOperation(args)\n",
    "weak_disjunction_op = WeakDisjunctionOperation(args)\n",
    "strong_conjunction_op = StrongConjunctionOperation(args)\n",
    "strong_disjunction_op = StrongDisjunctionOperation(args)\n",
    "\n",
    "# 测试 split_input 方法\n",
    "def test_split_input(operation):\n",
    "    test_input = torch.randn(2, args.in_channels).to(args.device)\n",
    "    x1, x2 = operation.split_input(test_input)\n",
    "    assert x1.shape == (2, args.in_channels // 2), f\"Expected shape: {(2, args.in_channels // 2)}, but got {x1.shape}\"\n",
    "    assert x2.shape == (2, args.in_channels // 2), f\"Expected shape: {(2, args.in_channels // 2)}, but got {x2.shape}\"\n",
    "    print(f\"split_input test passed for {operation.name}\")\n",
    "\n",
    "# 测试 forward 方法\n",
    "def test_forward(operation):\n",
    "    operation.test_forward()\n",
    "    print(f\"forward test passed for {operation.name}\")\n",
    "\n",
    "# 运行测试\n",
    "for op in [implication_op, equivalence_op, negation_op, weak_conjunction_op, weak_disjunction_op, strong_conjunction_op, strong_disjunction_op]:\n",
    "    if op is not negation_op:  # negation_op 为一元操作，不需要 split_input 方法\n",
    "        test_split_input(op)\n",
    "    test_forward(op)\n",
    "\n",
    "print(\"All tests passed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LQ1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
